{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of segmentation to btrack to napari visualization\n",
    "\n",
    "This example uses TIF files saved out from segmentation using *stardist3D*, although will work for other segmentation pipelines too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import btrack\n",
    "import imageio\n",
    "import napari\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/media/quantumjot/Data/Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 1 - using a numpy array\n",
    "\n",
    "In this example, each image from the timelapse is a 3D volume (32 x 1200 x 1200) and there are 10 timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_arr(path):\n",
    "    \"\"\"Segmentation as numpy array.\"\"\"\n",
    "    \n",
    "    stack = []\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # make sure you sort the filenames correctly here\n",
    "    files.sort()\n",
    "    \n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        \n",
    "        # note, change this to imageio.imread for 2D data\n",
    "        img = imageio.volread(os.path.join(path, filename))\n",
    "        stack.append(img)\n",
    "        \n",
    "    return np.stack(stack, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_0.tif\n",
      "labels_1.tif\n",
      "labels_2.tif\n",
      "labels_3.tif\n",
      "labels_4.tif\n",
      "labels_5.tif\n",
      "labels_6.tif\n",
      "labels_7.tif\n",
      "labels_8.tif\n",
      "labels_9.tif\n"
     ]
    }
   ],
   "source": [
    "stack = segmentation_arr(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print out the shape of the stack (T, Z, Y, X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 1200, 1200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - using a generator\n",
    "\n",
    "This is useful if you're resource constrained and don't want to load all of the image data, or they are stored in an unusual format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_generator(path):\n",
    "    \"\"\"Segmentation generator\"\"\"\n",
    "    files = os.listdir(path)\n",
    "    files.sort()\n",
    "    \n",
    "    for filename in files:\n",
    "        \n",
    "        # note, change this to imageio.imread for 2D data\n",
    "        img = imageio.volread(os.path.join(path, filename))\n",
    "        \n",
    "        yield img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = segmentation_generator(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## localizing the objects\n",
    "\n",
    "Now we use a utility function to localise the objects in the segmentation, and also apply anisotropic scaling (using the `scale` option, here the z-values are scaled by 2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2021/04/19 12:46:47 PM] Localizing objects from segmentation...\n",
      "[INFO][2021/04/19 12:46:53 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2021/04/19 12:46:53 PM] ...Found 11625 objects in 10 frames.\n"
     ]
    }
   ],
   "source": [
    "obj_from_arr = btrack.utils.segmentation_to_objects(stack, properties=('area', ), scale=(2., 1., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 939.0, 'y': 20.0, 'z': 12.0, 't': 0, 'dummy': False, 'states': 0, 'label': 5, 'prob': 0.0, 'area': 1381}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the first object\n",
    "obj_from_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2021/04/19 12:46:53 PM] Localizing objects from segmentation...\n",
      "[INFO][2021/04/19 12:47:04 PM] Objects are of type: <class 'dict'>\n",
      "[INFO][2021/04/19 12:47:04 PM] ...Found 11625 objects in 10 frames.\n"
     ]
    }
   ],
   "source": [
    "obj_from_generator = btrack.utils.segmentation_to_objects(\n",
    "    generator, \n",
    "    properties = ('area', 'major_axis_length'), \n",
    "    scale=(2., 1., 1.)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>t</th>\n",
       "      <th>dummy</th>\n",
       "      <th>states</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>area</th>\n",
       "      <th>major_axis_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1381</td>\n",
       "      <td>26.792221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "{'ID': 0, 'x': 939.0, 'y': 20.0, 'z': 12.0, 't': 0, 'dummy': False, 'states': 0, 'label': 5, 'prob': 0.0, 'area': 1381, 'major_axis_length': 26.792220587380353}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the first object\n",
    "obj_from_generator[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run btrack with the objects\n",
    "\n",
    "We will use the objects from the generator here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2021/04/19 12:47:04 PM] Loaded btrack: /home/quantumjot/Dropbox/Code/py3/BayesianTracker/btrack/libs/libtracker.so\n",
      "[INFO][2021/04/19 12:47:04 PM] btrack (v0.4.1) library imported\n",
      "[INFO][2021/04/19 12:47:04 PM] Setting max XYZ search radius to: 100\n",
      "[INFO][2021/04/19 12:47:04 PM] Starting BayesianTracker session\n",
      "[INFO][2021/04/19 12:47:04 PM] Loading configuration file: ../models/cell_config.json\n",
      "[INFO][2021/04/19 12:47:04 PM] Loading motion model: b'cell_motion'\n",
      "[INFO][2021/04/19 12:47:04 PM] Setting max XYZ search radius to: 10\n",
      "[INFO][2021/04/19 12:47:04 PM] Objects are of type: <class 'list'>\n",
      "[INFO][2021/04/19 12:47:04 PM] Set volume to ((0, 1200), (0, 1200), (-100000.0, 64.0))\n",
      "[INFO][2021/04/19 12:47:04 PM] Starting tracking... \n",
      "[INFO][2021/04/19 12:47:05 PM] Tracking objects in frames 0 to 10 (of 10)...\n",
      "[INFO][2021/04/19 12:47:12 PM]  - Timing (Bayesian updates: 904.77ms, Linking: 8.45ms)\n",
      "[INFO][2021/04/19 12:47:12 PM]  - Probabilities (Link: 1.00000, Lost: 0.52636)\n",
      "[INFO][2021/04/19 12:47:12 PM] SUCCESS.\n",
      "[INFO][2021/04/19 12:47:12 PM]  - Found 1756 tracks in 10 frames (in 0.0s)\n",
      "[INFO][2021/04/19 12:47:12 PM]  - Inserted 241 dummy objects to fill tracking gaps\n",
      "[INFO][2021/04/19 12:47:12 PM] Loading hypothesis model: cell_hypothesis\n",
      "[INFO][2021/04/19 12:47:12 PM] Calculating hypotheses (relax: True)...\n",
      "[INFO][2021/04/19 12:47:12 PM] Setting up constraints matrix for global optimisation...\n",
      "[INFO][2021/04/19 12:47:12 PM] Optimizing...\n",
      "[INFO][2021/04/19 12:47:13 PM] Optimization complete. (Solution: optimal)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.FALSE_POSITIVE: 353 (of 1756)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.LINK: 124 (of 385)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.DIVIDE: 41 (of 111)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.INITIALIZE_BORDER: 22 (of 46)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.INITIALIZE_FRONT: 1124 (of 1434)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.INITIALIZE_LAZY: 51 (of 276)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.TERMINATE_BORDER: 29 (of 55)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.TERMINATE_BACK: 1147 (of 1329)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - Fates.TERMINATE_LAZY: 62 (of 372)\n",
      "[INFO][2021/04/19 12:47:13 PM]  - TOTAL: 5764 hypotheses\n",
      "[INFO][2021/04/19 12:47:13 PM] Completed optimization with 1632 tracks\n",
      "[INFO][2021/04/19 12:47:13 PM] Opening HDF file: /media/quantumjot/Data/Test/tracking.h5...\n",
      "[INFO][2021/04/19 12:47:13 PM] Writing objects/obj_type_1\n",
      "[INFO][2021/04/19 12:47:13 PM] Writing labels/obj_type_1\n",
      "[INFO][2021/04/19 12:47:13 PM] Writing properties/obj_type_1\n",
      "[INFO][2021/04/19 12:47:13 PM] Writing tracks/obj_type_1\n",
      "[INFO][2021/04/19 12:47:13 PM] Writing dummies/obj_type_1\n",
      "[INFO][2021/04/19 12:47:13 PM] Writing LBEP/obj_type_1\n",
      "[INFO][2021/04/19 12:47:13 PM] Writing fates/obj_type_1\n",
      "[INFO][2021/04/19 12:47:13 PM] Closing HDF file: /media/quantumjot/Data/Test/tracking.h5\n",
      "[INFO][2021/04/19 12:47:13 PM] Ending BayesianTracker session\n"
     ]
    }
   ],
   "source": [
    "# initialise a tracker session using a context manager\n",
    "with btrack.BayesianTracker() as tracker:\n",
    "\n",
    "    # configure the tracker using a config file\n",
    "    tracker.configure_from_file('../models/cell_config.json')\n",
    "    tracker.max_search_radius = 10\n",
    "\n",
    "    # append the objects to be tracked\n",
    "    tracker.append(obj_from_generator)\n",
    "\n",
    "    # set the volume\n",
    "    tracker.volume=((0, 1200), (0, 1200), (-1e5, 64.))\n",
    "\n",
    "    # track them (in interactive mode)\n",
    "    tracker.track_interactive(step_size=100)\n",
    "\n",
    "    # generate hypotheses and run the global optimizer\n",
    "    tracker.optimize()\n",
    "\n",
    "    tracker.export(os.path.join(PATH, 'tracking.h5'), obj_type='obj_type_1')\n",
    "\n",
    "    # get the tracks in a format for napari visualization\n",
    "    data, properties, graph = tracker.to_napari(ndim=3)\n",
    "    \n",
    "    tracks = tracker.tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize with napari\n",
    "\n",
    "Note that we also set the scale of the images here to account for the anisotropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tracks layer 'Tracks' at 0x7f071cbf1050>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_labels(stack, scale=(1., 2., 1., 1.), name='Segmentation')\n",
    "viewer.add_tracks(data, properties=properties, graph=graph, name='Tracks')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:napari]",
   "language": "python",
   "name": "conda-env-napari-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
